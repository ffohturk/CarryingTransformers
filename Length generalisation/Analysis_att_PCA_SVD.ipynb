{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "matplotlib.use(\"pgf\")\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from requests import get\n",
    "import zipfile, io\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from IPython import display\n",
    "import torch.nn.utils.parametrizations as param\n",
    "%matplotlib inline\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "plt.rcParams.update({\n",
    "        \"pgf.texsystem\": \"pdflatex\",\n",
    "        \"text.usetex\": True,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": \"Helvetica\",\n",
    "        'pgf.rcfonts': False,\n",
    "    })\n",
    "\n",
    "plt.style.use('seaborn-v0_8-ticks')\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definitions, can skip this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderTot(nn.Module):\n",
    "\n",
    "  def __init__(self, decoder, embed, generator):\n",
    "    super().__init__()\n",
    "    self.embed = embed\n",
    "    self.gen = generator\n",
    "    self.decoder = decoder\n",
    "    self.generator = generator\n",
    "\n",
    "  def forward(self, src, mask):\n",
    "    return self.generator(self.decoder(self.embed(src), mask))\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "  def __init__(self, d_model, vocab_size):\n",
    "    super().__init__()\n",
    "    self.ln = nn.Linear(d_model, vocab_size)\n",
    "    self.out = None\n",
    "\n",
    "  def forward(self, x):\n",
    "    self.out = self.ln(x)\n",
    "    return F.log_softmax(self.ln(x), dim=-1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, attn, ffn, d_model, dropout, ablation_data_dec):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.ffn = ffn\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.d_model = d_model\n",
    "        self.out = None\n",
    "        self.out_a = None\n",
    "        self.n_ab_att = ablation_data_dec[0]\n",
    "        self.n_ab_ffn = ablation_data_dec[1]\n",
    "        \n",
    "    def forward(self, x, mask, n):\n",
    "        \n",
    "        x1 = self.norm(x)\n",
    "\n",
    "        if n in self.n_ab_att:\n",
    "            x = x + 0*self.dropout(self.attn(x1, x1, x1, n, mask))\n",
    "        else:\n",
    "            x = x + self.dropout(self.attn(x1, x1, x1, n, mask))\n",
    "\n",
    "        self.out_a = x\n",
    "        \n",
    "        if n in self.n_ab_ffn:\n",
    "            self.out = x + 0*self.dropout(self.ffn(self.norm(x), n))\n",
    "        else:\n",
    "            self.out = x + self.dropout(self.ffn(self.norm(x), n))\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "class DecoderStack(nn.Module):\n",
    "    \n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.norm = nn.LayerNorm(layer.d_model)\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        n = 0\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask, n)\n",
    "            n += 1\n",
    "        return self.norm(x)\n",
    "\n",
    "def Attention(q, k, v, n, ablation_data_att, mask=None, dropout=None):\n",
    "            \n",
    "            M_apply, n_ab_head, ab_head, ab_head_row = ablation_data_att\n",
    "            \n",
    "            ### -- Softmax Attention -- ###\n",
    "            \n",
    "            # q, k, v are dims (batch_size, # heads, seq_len, d_{k,v}) \n",
    "            \n",
    "            m = torch.arange(k.shape[-2]).view(k.shape[-2], 1)\n",
    "            t = torch.arange(k.shape[-1]).view(1, k.shape[-1])\n",
    "            t = torch.exp( - ( 2 * np.log(10**4) / k.shape[-1] ) * torch.floor(t/2) )\n",
    "            r1 = torch.cos(m * t)\n",
    "            r2 = torch.sin(m * t)\n",
    "            \n",
    "            K = torch.cat((q, k, v))\n",
    "            \n",
    "            Kp = torch.einsum('ijkl, kl -> ijkl', K, r1)\n",
    "            \n",
    "            L = torch.kron(torch.eye(k.shape[-1]//2), torch.Tensor([[0,-1],[1,0]]))\n",
    "            K = torch.einsum('ijkl, ml -> ijkm', K, L)\n",
    "            \n",
    "            Kp += torch.einsum('ijkl, kl -> ijkl', K, r2)\n",
    "            \n",
    "            Kp = Kp.view(-1, k.shape[0], k.shape[1], k.shape[2], k.shape[-1])\n",
    "            \n",
    "            q, k, v = Kp[0], Kp[1], Kp[2]\n",
    "            \n",
    "            A = torch.matmul(q, k.transpose(-2,-1)) * k.size(-1)**(-0.5)\n",
    "            \n",
    "            if M_apply:\n",
    "\n",
    "                range_in = np.arange(A.shape[-1])\n",
    "                range_in_1 = np.delete(range_in, ab_head_row[n][0])\n",
    "                range_in_2 = np.delete(range_in, ab_head_row[n][1])\n",
    "                index_p1 = torch.tensor(range_in_1)\n",
    "                index_p2 = torch.tensor(range_in_2)\n",
    "                Ab_mask = torch.zeros_like(A)\n",
    "                Ab_mask[:, 0, :, :].index_fill_(-2, index_p1, 1)\n",
    "                Ab_mask[:, 1, :, :].index_fill_(-2, index_p2, 1)\n",
    "            \n",
    "            if mask is not None:\n",
    "                # mask = mask.unsqueeze(1)\n",
    "                A.masked_fill_(mask == 0, float('-inf'))\n",
    "\n",
    "            O = F.softmax(A, dim=-1)\n",
    "\n",
    "            if dropout is not None:\n",
    "                O = dropout(O)\n",
    "            \n",
    "            if n in n_ab_head and M_apply:\n",
    "                Ab = Ab_mask\n",
    "                O = torch.einsum('ijkl, ijkl -> ijkl', O, Ab)\n",
    "                \n",
    "            if n in n_ab_head:\n",
    "                Ab = ab_head[n]\n",
    "                O = torch.einsum('ijkl, j -> ijkl', O, Ab)\n",
    "            \n",
    "            return torch.matmul(O, v), O, A\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, h, d_model, ablation_data_attention, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.attn = None\n",
    "        self.attnA = None\n",
    "        self.out = None\n",
    "        self.out_A = None\n",
    "        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(4)])\n",
    "        self.dropout =  nn.Dropout(p=dropout)\n",
    "        self.ab = ablation_data_attention\n",
    "        \n",
    "    \n",
    "    def forward(self, query, keys, values, n, mask=None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        x = [l(z).view(batch_size, -1, self.h, self.d_k).transpose(1, 2) for l, z in zip(self.linears, (query, keys, values))]\n",
    "        \n",
    "        y, self.attn, self.attnA = Attention(x[0], x[1], x[2], n, ablation_data_att=self.ab, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        y = y.transpose(1,2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "        \n",
    "        self.out_A = y\n",
    "\n",
    "        self.out = self.linears[-1](y)\n",
    "\n",
    "        return self.linears[-1](y)\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout, ablation_data_ffn):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff)\n",
    "        self.w2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = None\n",
    "        self.out_p = None\n",
    "        self.FFN_needle = ablation_data_ffn\n",
    "    \n",
    "    def forward(self, x, n):\n",
    "        A = self.w1(x)\n",
    "        if n == 1:\n",
    "            for i in range(A.shape[-1]):\n",
    "                if i in self.FFN_needle:\n",
    "                    A[:, :, i] *= 0\n",
    "        self.out = self.relu(A)\n",
    "        self.out_p = self.w2(self.dropout(self.out))\n",
    "        return self.w2(self.dropout(self.out))\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_vocab, d_model):\n",
    "        super().__init__()\n",
    "        self.Emb = nn.Embedding(src_vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.out_e = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.out_e = self.Emb(x) * np.sqrt(self.d_model)\n",
    "        return self.Emb(x) * np.sqrt(self.d_model)\n",
    "\n",
    "def make_model(vocab, N, d_model, d_ff, h, dropout, ablation_data):\n",
    "    \n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model, ablation_data[0])\n",
    "    ffn = FeedForward(d_model, d_ff, dropout, ablation_data[1])\n",
    "    \n",
    "    model = DecoderTot(DecoderStack(Decoder( c(attn), c(ffn), d_model, dropout, ablation_data[2]), N),\n",
    "                           Embeddings(vocab, d_model),  Generator(d_model, vocab))\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1: # This is there to not initialize the biases\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to construct dataset and label it according to where the carrying happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\n",
    "Generate the dataset for ndig digits (with 0 padding) and ndig + n_extra digits \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def GenerateDataset(ndig, n_extra):\n",
    "\n",
    "    P = 10**ndig\n",
    "    \n",
    "    data = []\n",
    "    target = []\n",
    "\n",
    "    stoi = {'0': 0, '1': 1, '2': 2,'3': 3,'4': 4,'5': 5,'6': 6,'7': 7,'8': 8,'9': 9,'+': 10,'=': 11}\n",
    "\n",
    "    for i in range(P):\n",
    "        for j in range(P):\n",
    "            if i + j < P:\n",
    "                li = list(f'{i}')\n",
    "                lj = list(f'{j}')\n",
    "                lij = list(f'{i+j}')\n",
    "                if len(li) < ndig + n_extra:\n",
    "                    li = ['0'] * (ndig + n_extra - len(li)) + li\n",
    "                if len(lj) < ndig + n_extra:\n",
    "                    lj = ['0'] * (ndig + n_extra - len(lj)) + lj\n",
    "                if len(lij) < ndig + n_extra:\n",
    "                    lij = ['0'] * (ndig + n_extra - len(lij)) + lij\n",
    "\n",
    "                lsum = li + ['+'] + lj + ['='] * (ndig + n_extra)\n",
    "                # lij = ['0'] * (2*(ndig + 3) + 1) + lij\n",
    "                data.append([stoi[lsum[i]] for i in range(len(lsum))])\n",
    "                target.append([stoi[lij[i]] for i in range(len(lij))])\n",
    "\n",
    "    vocab = len(stoi) \n",
    "    data = torch.LongTensor(data)\n",
    "    target = torch.LongTensor(target)\n",
    "\n",
    "    data_f = []\n",
    "    target_f = []\n",
    "\n",
    "    P_f = 10**(ndig + n_extra) - 1\n",
    "\n",
    "    k = 0\n",
    "    while k < 20000:\n",
    "        i = torch.randint(P_f, size=(1,)).item()\n",
    "        j = torch.randint(P_f, size=(1,)).item()\n",
    "        if i + j < P_f + 1:\n",
    "            li = list(f'{i}')\n",
    "            lj = list(f'{j}')\n",
    "            lij = list(f'{i+j}')\n",
    "            if len(li) < ndig + n_extra:\n",
    "                li = ['0'] * (ndig + n_extra - len(li)) + li\n",
    "            if len(lj) < ndig + n_extra:\n",
    "                lj = ['0'] * (ndig + n_extra - len(lj)) + lj\n",
    "            if len(lij) < ndig + n_extra:\n",
    "                lij = ['0'] * (ndig + n_extra - len(lij)) + lij\n",
    "\n",
    "            lsum = li + ['+'] + lj + ['='] * (ndig + n_extra)\n",
    "            # lij = ['0'] * (2*(ndig + 3) + 1) + lij\n",
    "            data_f.append([stoi[lsum[i]] for i in range(len(lsum))])\n",
    "            target_f.append([stoi[lij[i]] for i in range(len(lij))])\n",
    "            k += 1\n",
    "\n",
    "    data_f = torch.LongTensor(data_f)\n",
    "    target_f = torch.LongTensor(target_f)\n",
    "\n",
    "    return vocab, data, target, data_f, target_f\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "Construct a label for each example, based on whether the sum is >= 10, < 9 or ==9. The label is an integer constructed as follows. We start with an integer s = 0.\n",
    "At position i, if the sum at this position is < 9, then we add 0 to s. if it is >= 10, we add 10**i to s and if it is equal to 9 (and the previousu sum was >= 10),\n",
    "then we add 2*10**i to s. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def Pick_classes(data, ndig):\n",
    "    s = 0\n",
    "    if data[ndig - 1] + data[2*ndig] >= 10:\n",
    "        r = 1\n",
    "        s += 1\n",
    "    else:\n",
    "        r = 0\n",
    "        s += 0\n",
    "\n",
    "    for i in range(1, ndig):\n",
    "\n",
    "        if data[ndig - i - 1] + data[2*ndig - i] >= 10:\n",
    "            r = 1\n",
    "            s += 10**i\n",
    "        elif (data[ndig - 1 - i] + data[2*ndig - i] == 9) and (r == 1):\n",
    "            r = 1\n",
    "            s += 2*10**i\n",
    "        else:\n",
    "            r = 0\n",
    "            s += 0\n",
    "    return s \n",
    "\n",
    "\"\"\" \n",
    "\n",
    "Find indices of the target integer with a given output at a given position.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def Pick_target_classes(target):\n",
    "    idx = []\n",
    "    for i in range(target.shape[-1]):\n",
    "        ids_p = []\n",
    "        for j in range(10):\n",
    "            ids_p.append(torch.argwhere(target[:, i] == j).view(-1))\n",
    "        idx.append(ids_p)\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get ablation data\n",
    "\n",
    "<font size = '3'> Specify what component in what layer to ablate. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ablation specification\n",
    "\n",
    "## Attention \n",
    "\n",
    "### The function generate_head_layer_ablations generates the set of all possible combinations of heads that can be ablated. \n",
    "# Pick one of these an use it for ab_head. Then specify with n_ab_head what layer you want to apply that ablation to. \n",
    "# The list ab_headrow is used in conjuction with M_apply. If M_apply is true, ab_headrow will be used and will ablate a specific row of the attention pattern.\n",
    "# Again n_ab_head determines what layer this ablation is applied to. \n",
    "\n",
    "n_ab_head = [] # Specifies of what layer we want to apply zero-ablation to.\n",
    "ab_head = torch.tensor([[1, 1], [1, 1]]) # Specifies what head to ablate. This is a tensor of size (n, heads) with a 0 for those heads that you want to ablate, otherwise entries are one.\n",
    "ab_headrow = [] # Specifies what row within a head we want to apply zero-ablation to.\n",
    "M_apply = False # Specifies to whether to apply row-wise ablation or not. If True, use n_ab_head to set what layers you want the row-wise ablation to apply to.\n",
    "\n",
    "\n",
    "ablation_attention = [M_apply, n_ab_head, ab_head, ab_headrow]\n",
    "\n",
    "# FFN\n",
    "\n",
    "List_Neurons = [] # List of neurons in the final layer you want to ablate.\n",
    "\n",
    "ablation_ffn = List_Neurons\n",
    "\n",
    "## Decoder \n",
    "\n",
    "n_ab_ffn = [] # Specifies which layer you want to ablate the FFN of. \n",
    "n_ab_att = [] # Specifies which layer you want to ablate the entire attention of. \n",
    "\n",
    "ablation_decoder = [n_ab_att, n_ab_ffn]\n",
    "\n",
    "ablations = [ablation_attention, ablation_ffn, ablation_decoder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model\n",
    "\n",
    "<font size = \"3\"> Change directory and mmd to get the model you want to load, default is the primed model. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2 # Number of layers\n",
    "s = 0.3 # Train/test split\n",
    "w = 0.2 # weight decay\n",
    "p = 1 # Specifies which of the six models to consider. In this block used as a dummy variable, but can be specified later on. \n",
    "\n",
    "vocab = 12\n",
    "\n",
    "d_ff = 128\n",
    "d_model = 128\n",
    "heads = 2\n",
    "\n",
    "ndig = 3\n",
    "ntot = 6\n",
    "\n",
    "directory = 'finetuning/'.format(n, s, w)\n",
    "mdd = 'model_n{!s}_s{!s}_w{!s}_{!s}_FT_p_epoch_500'.format(n, s, w, p)\n",
    "toLoad = directory + mdd\n",
    "\n",
    "n_layer = n # number of layers\n",
    "d_model = 128 # model dimension, residual stream\n",
    "d_ff = d_model # dim intermediate feed-forward layer\n",
    "h_a = 2 # number of heads in attention (doesnt impact # of params)\n",
    "\n",
    "model = make_model(vocab, N = n_layer, d_model = d_model, d_ff = d_ff, h = h_a, dropout=0.1, ablation_data=ablations)\n",
    "model.load_state_dict(torch.load(toLoad, map_location='cpu')['model'])\n",
    "\n",
    "mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2 # Number of layers\n",
    "s = 0.3 # Train/test split\n",
    "w = 0.2 # weight decay\n",
    "p = 0 # Specifies which of the six models to consider. In this block used as a dummy variable, but can be specified later on. \n",
    "\n",
    "vocab = 12\n",
    "\n",
    "d_ff = 128\n",
    "d_model = 128\n",
    "heads = 2\n",
    "\n",
    "ndig = 3\n",
    "ntot = 6\n",
    "\n",
    "directory = 'unprimed/'.format(n, s, w)\n",
    "mdd = 'model_n{!s}_s{!s}_w{!s}_{!s}_epoch_500'.format(n, s, w, p)\n",
    "toLoad = directory + mdd\n",
    "\n",
    "n_layer = n # number of layers\n",
    "d_model = 128 # model dimension, residual stream\n",
    "d_ff = d_model # dim intermediate feed-forward layer\n",
    "h_a = 2 # number of heads in attention (doesnt impact # of params)\n",
    "\n",
    "modelp = make_model(vocab, N = n_layer, d_model = d_model, d_ff = d_ff, h = h_a, dropout=0.1, ablation_data=ablations)\n",
    "modelp.load_state_dict(torch.load(toLoad, map_location='cpu')['model'])\n",
    "\n",
    "mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- layer 0 ---\n",
      "tensor(14)\n",
      "tensor(8)\n",
      "tensor(3)\n",
      "tensor(4)\n",
      "--- layer 1 ---\n",
      "tensor(7)\n",
      "tensor(10)\n",
      "tensor(11)\n",
      "tensor(3)\n"
     ]
    }
   ],
   "source": [
    "for i in range(n):\n",
    "    print(f'--- layer {i} ---')\n",
    "    for j in range(4):\n",
    "\n",
    "        att = modelp.decoder.layers[i].attn.linears[j].weight.detach()\n",
    "\n",
    "        att_ft = model.decoder.layers[i].attn.linears[j].weight.detach()\n",
    "\n",
    "        zero_modes = (torch.abs((att - att_ft)) < 10**-6).sum()\n",
    "\n",
    "        print(zero_modes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data and compute accuracy on the ntot digit sums, i.e. we use the datasets data_f and target_f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, data, target, data_f, target_f = GenerateDataset(ndig, ntot - ndig)\n",
    "\n",
    "inputs = data_f[:20000]\n",
    "targets = target_f[:20000]\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    out = model(inputs, None)\n",
    "    out_p = torch.argmax(out, -1)\n",
    "\n",
    "pre_acc = (out_p[:, -ntot:] == targets[:, -ntot:])\n",
    "\n",
    "print('--- Accuracy per position ---')\n",
    "print((sum(pre_acc[i] for i in range(len(out_p))) / len(out_p)))\n",
    "print('--- Accuracy (correctness of target) --- ')\n",
    "print((sum(pre_acc[i].min() for i in range(len(out_p))) / len(out_p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a dictionary with keys being the type of sum (where the carry is etc.) and the values are the positions in the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_pos = {}\n",
    "\n",
    "ix = torch.randint(0, len(data_f), size=(20000,))\n",
    "data_p = data_f[ix]\n",
    "target_p = target_f[ix]\n",
    "\n",
    "for i, src in enumerate(data_p):\n",
    "    cls_ = Pick_classes(src, ntot)\n",
    "\n",
    "    if cls_ not in d_pos.keys():\n",
    "        d_pos[cls_] = [i]\n",
    "    elif cls_ in d_pos.keys():\n",
    "        d_pos[cls_].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the attention patterns for Fig. 21, 23 and 25.\n",
    "\n",
    "<font size = '3'> Make sure to select the primed model to reproduce Fig. 23, and the unprimed model for Fig. 21, and the finetuned model for Fig. 25. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = [1, 100, 210, 1110, 10010]\n",
    "\n",
    "colors = ['Blues', 'Oranges', 'Greens', 'Reds', 'Purples']\n",
    "\n",
    "fig, ax = plt.subplots(2*n, 5, figsize=(16, 20))\n",
    "fig.tight_layout(h_pad=-30, w_pad=-1)\n",
    "\n",
    "out = model(data_p, None)\n",
    "\n",
    "rows = []\n",
    "for i in range(len(positions)):\n",
    "    \n",
    "    for l in range(n):\n",
    "\n",
    "        att0 = model.decoder.layers[l].attn.attn[d_pos[positions[i]], :, :, :].clone().detach().mean(0)\n",
    "        if l == n-1: \n",
    "            ax[0 + 2*l, i].imshow(att0[0, -ntot:, :], cmap=colors[i])\n",
    "            ax[1 + 2*l, i].imshow(att0[1, -ntot:, :], cmap=colors[i])\n",
    "        else:\n",
    "            ax[0 + 2*l, i].imshow(att0[0, :, :], cmap=colors[i])\n",
    "            ax[1 + 2*l, i].imshow(att0[1, :, :], cmap=colors[i])\n",
    "        if l == 0:\n",
    "            rows.extend(['$\\\\rm Head\\;0\\\\hspace{-5pt}:\\\\hspace{-5pt}0$', '$\\\\rm Head\\;0\\\\hspace{-5pt}:\\\\hspace{-5pt}1$'])\n",
    "        elif l == 1:\n",
    "            rows.extend(['$\\\\rm Head\\;1\\\\hspace{-5pt}:\\\\hspace{-5pt}0$', '$\\\\rm Head\\;1\\\\hspace{-5pt}:\\\\hspace{-5pt}1$'])\n",
    "\n",
    "ax[0, 0].set_title('$\\\\texttt{000001}$')\n",
    "ax[0, 1].set_title('$\\\\texttt{000100}$')\n",
    "ax[0, 2].set_title('$\\\\texttt{000210}$')\n",
    "ax[0, 3].set_title('$\\\\texttt{001110}$')    \n",
    "ax[0, 4].set_title('$\\\\texttt{010010}$')\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(4):  \n",
    "        ax[j, i].set_xticks([])\n",
    "        ax[j, i].set_yticks([])\n",
    "for j in range(4):\n",
    "    if j < 2:\n",
    "        ax[j, 0].set_yticks(range(3*ntot+1), ['$*$']*ntot + ['$+$'] + ['$*$']*ntot + ['$=$'] * ntot)\n",
    "    else:\n",
    "        ax[j, 0].set_yticks(range(ntot), ['$=$']*ntot)  \n",
    "for i in range(5):\n",
    "    ax[-1, i].set_xticks(range(3*ntot+1), ['$*$']*ntot + ['$+$'] + ['$*$']*ntot + ['$=$'] * ntot) \n",
    "\n",
    "for ax, row in zip(ax[:,0], rows):\n",
    "    ax.annotate(row, xy=(0, 0.5), xytext=(-ax.yaxis.labelpad - 5, 0),\n",
    "                xycoords=ax.yaxis.label, textcoords='offset points',\n",
    "                size='large', ha='right', va='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the PCA of a given layer and after a given component.  Can be used to reproduce Fig. 24 and Fig. 22 and the claims about the finetuned model.\n",
    "\n",
    "<font size ='3'> Layers can be selected in the usual way. Use the attribute 'out_a' for the output of the attention (default) and the attribute 'out' for the output of the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlist = sorted(list(d_pos.keys()))\n",
    "lg = []\n",
    "\n",
    "inputs = data_p\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    out = model(inputs, None)\n",
    "\n",
    "Out_a = model.decoder.layers[1].out_a.detach().clone()\n",
    "Out_a = Out_a - Out_a.mean(0, keepdim=True)\n",
    "\n",
    "pca = [torch.svd(Out_a[:, k, :]) for k in range(3*ntot+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots PCA for the last ntot positions for the specified hidden states from above. Labelled according to the type of sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, ntot, figsize=(22, 3))\n",
    "fig.tight_layout(h_pad=-1, w_pad=1)\n",
    "\n",
    "a, b = 0, 1\n",
    "lg = []\n",
    "for i, pos in enumerate([1, 100, 210, 1110, 10010]):\n",
    "    for j in range(2*ntot+1, 3*ntot+1):\n",
    "        x = pca[j][0][d_pos[pos], a] * pca[j][1][a]\n",
    "        y = pca[j][0][d_pos[pos], b] * pca[j][1][b]\n",
    "\n",
    "        ax[j-2*ntot-1].scatter(x, y, alpha=0.3)\n",
    "    q = len(str(pos))\n",
    "    t = ''.join(['0']*(ntot - q) + str(pos).split())\n",
    "    lg.append(f'$\\\\texttt{t}$')\n",
    "\n",
    "\n",
    "lg = fig.legend(lg, bbox_to_anchor=(1.06, 0.24), loc='lower right', borderaxespad=0.)\n",
    "for handle in lg.legend_handles:\n",
    "    handle.set_alpha(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots PCA for the last ntot positions for the specified hidden states from above. Labelled according to the value of the target digit. Can be used to reproduce Fig. 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_classes = Pick_target_classes(target_p)\n",
    "\n",
    "u = []\n",
    "for pos in [1, 100, 210, 1110, 10010]:\n",
    "    u += d_pos[pos]\n",
    "\n",
    "fig, ax = plt.subplots(1, 6, figsize=(22, 3))\n",
    "fig.tight_layout(h_pad=-1, w_pad=1)\n",
    "\n",
    "a, b = 0, 1\n",
    "lg = []\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',\n",
    "                    '#9467bd', '#8c564b', '#e377c2', '#7f7f7f',\n",
    "                    '#bcbd22', '#17becf']\n",
    "\n",
    "q = 0\n",
    "for j in range(13, 19):\n",
    "    for i in range(10):\n",
    "        tgt_class = tgt_classes[j-13][i]\n",
    "        intersection = list(set(tgt_class.tolist()).intersection(set(u)))\n",
    "        x = pca[j][0][intersection, a] * pca[j][1][a]\n",
    "        y = pca[j][0][intersection, b] * pca[j][1][b]\n",
    "\n",
    "        ax[j-13].scatter(x, y, alpha=0.3, color=colors[i])\n",
    "    # q = len(str(s))\n",
    "    # t = ''.join(['0']*(6 - q) + str(s).split())\n",
    "    # lg.append(f'$\\\\texttt{t}$')\n",
    "    \n",
    "lg = plt.legend(['$0$', '$1$', '$2$', '$3$', '$4$', '$5$', '$6$', '$7$', '$8$', '$9$'], bbox_to_anchor=(1.4, 0.30), loc='lower right', borderaxespad=0.)\n",
    "for handle in lg.legend_handles:\n",
    "    handle.set_alpha(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code below can be used to generate the SVD of the preactivations, labelled by whether they are activated or not, depending on position and type of sum. (for six digit sum, there are six rows and 6 rows for the 6 types of sums we consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This generates a scatter plot of 64 most active neurons for a given task and position.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "number_most_active_neurons = 64 # Number of most active neurons to consider\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Get preactivation weights and perform SVD\n",
    "\n",
    "MLP = model.decoder.layers[1].ffn.w1.weight.clone().detach()\n",
    "svd_mlp = torch.svd(MLP)\n",
    "\n",
    "positions = []\n",
    "\n",
    "# The types of sums to consider and collect positions in full dataset.\n",
    "\n",
    "for case in [0, 1, 10, 11, 21, 1100]:\n",
    "\n",
    "    positions.append(d_pos[case])\n",
    "\n",
    "# Label target data according to target digit at given position.\n",
    "digit_ans_pos = []\n",
    "for k in range(6):\n",
    "    ans_pp = []\n",
    "    for i in range(10):\n",
    "        ans_p = []\n",
    "        for j in range(len(target_p)):\n",
    "            if target_p[j, k] == i:\n",
    "                ans_p.append(j)\n",
    "        ans_pp.append(torch.tensor(ans_p))\n",
    "    digit_ans_pos.append(ans_pp)\n",
    "\n",
    "out = model(data_p, None)\n",
    "\n",
    "z_out = []\n",
    "for k in range(6): \n",
    "    z_out_p = []\n",
    "    for i in range(10):\n",
    "        # Computes activations for examples labelled by their target digit at a given output position.\n",
    "        z = model.decoder.layers[1].ffn.out[digit_ans_pos[k][i], 13+k, :].clone().detach()\n",
    "        z_out_p.append(z)\n",
    "    z_out.append(z_out_p)\n",
    "\n",
    "z_out_n = []\n",
    "for k in range(6): \n",
    "    z_out_p = []\n",
    "    for i in range(len(positions)):\n",
    "        # Computes activations for examples labelled by the type of sum at a given output position.\n",
    "        z = model.decoder.layers[1].ffn.out[positions[i], 13+k, :].clone().detach()\n",
    "        z_out_p.append(z)\n",
    "    z_out_n.append(z_out_p)\n",
    "\n",
    "def counter(a):\n",
    "    \"\"\"Counts number of occurences of particular elements in tensor. \n",
    "\n",
    "    Args:\n",
    "        a (torch.tensor): a tensor\n",
    "\n",
    "    Returns:\n",
    "       torch.tensor: tensor consisting of pairs of element with frequency.\n",
    "    \"\"\"\n",
    "    b = a.sort(descending=True)[0]\n",
    "    s = []\n",
    "    j = 1\n",
    "    for i in range(b.shape[0]-1):\n",
    "        if (b[i] > b[i+1]) and (i != b.shape[0] - 2):\n",
    "            s.append([b[i], j])\n",
    "            j = 1\n",
    "        elif i == b.shape[0] - 2:\n",
    "            s.append([b[i], j + 1])\n",
    "        elif b[i] == b[i+1]: \n",
    "            j += 1\n",
    "        \n",
    "    return torch.tensor(s)\n",
    "\n",
    "fig, ax = plt.subplots(6, 6, figsize=(20, 10))\n",
    "\n",
    "for k in range(6):\n",
    "    for j in range(len(positions)):\n",
    "        zz = torch.argwhere(z_out_n[k][j] != 0)[:, 1] # Get active neurons.\n",
    "\n",
    "        # Count the number of times a active neurons is activated.\n",
    "        s_ = counter(zz) \n",
    "\n",
    "        # Get only the top 64 most activated neurons.\n",
    "        sp = s_[s_[:, 1].sort(descending=True)[1]][:number_most_active_neurons, 0]\n",
    "\n",
    "        # Active neurons\n",
    "        zzpos = list(set(sp.tolist()))\n",
    "        # Non-active neurons\n",
    "        notzzpos = list(set(range(d_ff)) - set(zzpos))\n",
    "       \n",
    "        a, b = 0, 1\n",
    "\n",
    "        x = svd_mlp[0][notzzpos, a] * svd_mlp[1][a]\n",
    "        y = svd_mlp[0][notzzpos, b] * svd_mlp[1][b]\n",
    "\n",
    "        ax[5-k, j].scatter(x, y, alpha=0.3, color='blue')\n",
    "\n",
    "        x = svd_mlp[0][zzpos, a] * svd_mlp[1][a]\n",
    "        y = svd_mlp[0][zzpos, b] * svd_mlp[1][b]\n",
    "\n",
    "        ax[5-k, j].scatter(x, y, alpha=0.3, color='red')\n",
    "        ax[5-k, j].set_yticks([])\n",
    "        ax[5-k, j].set_xticks([])\n",
    "\n",
    "        # x = svd1[0][L, a] * svd1[1][a]\n",
    "        # y = svd1[0][L, b] * svd1[1][b]\n",
    "\n",
    "        # ax[j, k].scatter(x, y, alpha=0.3, color='black', marker='x')\n",
    "    for i, txt in enumerate([0, 1, 10, 11, 21, 1100]):\n",
    "        ax[0, i].set_title(f'$\\\\texttt{txt}$')\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
