{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed6ff40-26e8-48d5-aac4-f0b9c18af798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "matplotlib.use(\"pgf\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from requests import get\n",
    "import zipfile, io\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "from model import make_model\n",
    "from dataset_generator import dataset_generator\n",
    "from utilities import generate_head_layer_ablations, svd, PCA_plot, plot_attention_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad760e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "plt.rcParams.update({\n",
    "        \"pgf.texsystem\": \"pdflatex\",\n",
    "        \"text.usetex\": True,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": \"Helvetica\",\n",
    "        'pgf.rcfonts': False,\n",
    "    })\n",
    "\n",
    "plt.style.use('seaborn-v0_8-ticks')\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "\n",
    "# plt.locator_params(nbins=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89d4cca",
   "metadata": {},
   "source": [
    "# Decoder only models analysis\n",
    "\n",
    "<font size = '3'> Below is how we dealt with the different kinds of ablations. It is a bit cumbersome, but hopefully you will manage.  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10509ef8-57f0-4e1c-b771-2afbefdac338",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ablation specification\n",
    "\n",
    "## Attention \n",
    "\n",
    "### The function generate_head_layer_ablations generates the set of all possible combinations of heads that can be ablated. \n",
    "# Pick one of these an use it for ab_head. Then specify with n_ab_head what layer you want to apply that ablation to. \n",
    "# The list ab_headrow is used in conjuction with M_apply. If M_apply is true, ab_headrow will be used and will ablate a specific row of the attention pattern.\n",
    "# Again n_ab_head determines what layer this ablation is applied to. \n",
    "\n",
    "n_ab_head = [] # Specifies of what layer we want to apply zero-ablation to.\n",
    "ab_head = [[1, 1], [1, 1]] # Specifies what head to ablate. This is a tensor of size (n, heads) with a 0 for those heads that you want to ablate, otherwise entries are one.\n",
    "ab_headrow = [] # Specifies what row within a head we want to apply zero-ablation to.\n",
    "M_apply = False # Specifies to whether to apply row-wise ablation or not. If True, use n_ab_head to set what layers you want the row-wise ablation to apply to.\n",
    "\n",
    "\n",
    "ablation_attention = [M_apply, n_ab_head, ab_head, ab_headrow]\n",
    "\n",
    "# FFN\n",
    "\n",
    "List_Neurons = [] # List of neurons in the final layer you want to ablate.\n",
    "\n",
    "ablation_ffn = List_Neurons\n",
    "\n",
    "## Decoder \n",
    "\n",
    "n_ab_ffn = [] # Specifies which layer you want to ablate the FFN of. \n",
    "n_ab_att = [] # Specifies which layer you want to ablate the entire attention of. \n",
    "\n",
    "ablation_decoder = [n_ab_att, n_ab_ffn]\n",
    "\n",
    "ablations = [ablation_attention, ablation_ffn, ablation_decoder]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3b815e",
   "metadata": {},
   "source": [
    "Data generation, output is input data, target data and the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923c2396",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f, target_f, stoi = dataset_generator(P_f = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a99175",
   "metadata": {},
   "source": [
    "Load the model with specified layers (n), train/test split (s) and weight decay (w)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16defa8-3091-4a54-91d0-e7f0aa3521ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "s = 0.3\n",
    "w = 0.2\n",
    "\n",
    "vocab = 12\n",
    "\n",
    "d_model = 128\n",
    "d_ff = 128\n",
    "heads = 2\n",
    "dropout = 0.1\n",
    "\n",
    "directory = 'n{!s}_s{!s}_w{!s}/'.format(n, s, w)\n",
    "mdd = 'model_n{!s}_s{!s}_w{!s}'.format(n, s, w)\n",
    "toLoad = directory + mdd\n",
    "\n",
    "model = make_model(vocab, N = n, d_model = d_model, d_ff = d_ff, h = heads, dropout = dropout, ablation_data = ablations)\n",
    "model.load_state_dict(torch.load(toLoad, map_location='cpu')[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "print('# of parameters =', sum(p.nelement() for p in model.parameters())) # number of parameters in total\n",
    "print('Training set size =', len(data_f)*s)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6630f718",
   "metadata": {},
   "source": [
    "Generates all possible ablations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2399120",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = generate_head_layer_ablations(n=n, heads=heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daab16f",
   "metadata": {},
   "source": [
    "# Reproducing Figures 16, 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4807b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This outputs:\n",
    "\n",
    "    1) PCA Analysis at output of Attention and MLP in each layer for the two leading axes. \n",
    "    \n",
    "    2) Attention pattern per task (determined by where carried ones are needed)\n",
    "\"\"\"\n",
    "\n",
    "### Ablation specification\n",
    "\n",
    "## Attention \n",
    "\n",
    "M_apply = False\n",
    "n_ab_head = []\n",
    "# ab_headrow = [[[7, 8, 9], [7, 8, 9]], [[0, 1, 2, 3, 4, 5, 6], [0, 1, 2, 3, 4, 5, 6]], [[0, 1, 2, 3, 4, 5, 6], [0, 1, 2, 3, 4, 5, 6]]]\n",
    "# ab_headrow = [[[7, 8, 9], [7, 8, 9]], [[],[]]]\n",
    "ab_headrow = []\n",
    "ab_head = torch.eye(2,2)\n",
    "\n",
    "ablation_attention = [M_apply, n_ab_head, ab_head, ab_headrow]\n",
    "\n",
    "## FFN\n",
    "\n",
    "List_Neurons = []\n",
    "\n",
    "ablation_ffn = List_Neurons\n",
    "\n",
    "## Decoder \n",
    "\n",
    "n_ab_ffn = []\n",
    "n_ab_att = []\n",
    "\n",
    "ablation_decoder = [n_ab_att, n_ab_ffn]\n",
    "\n",
    "ablations = [ablation_attention, ablation_ffn, ablation_decoder]\n",
    "\n",
    "ix = torch.randint(len(data_f), size = (20000,))\n",
    "data_ff = data_f[ix, :]\n",
    "target_ff = target_f[ix, -4:-1]\n",
    "\n",
    "toLoad = directory + mdd\n",
    "\n",
    "model = make_model(vocab, N = n, d_model = 128, d_ff = d_ff, h = 2, dropout=0.1, ablation_data=ablations)\n",
    "model.load_state_dict(torch.load(toLoad, map_location='cpu')[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "seq_len = data_ff.shape[-1]\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "\n",
    "out = model(data_ff.to('cpu'), mask)\n",
    "\n",
    "Out_all = []\n",
    "for l in range(n):\n",
    "\n",
    "    Out_a = model.decoder.layers[l].out_a[:, :, :].detach().clone()\n",
    "    Out_a = Out_a - Out_a.mean(0, keepdim=True)\n",
    "    Out_all.append(Out_a)\n",
    "    Out_f = model.decoder.layers[l].out[:, :, :].detach().clone()\n",
    "    Out_f = Out_f - Out_f.mean(0, keepdim=True)\n",
    "    Out_all.append(Out_f)\n",
    "\n",
    "\n",
    "svd_full, positions, digit_ans_pos, digit_naive_ans_pos = svd(Out_all, data_ff, target_ff)\n",
    "\n",
    "PCA_plot(n=n, path=\"\", svd_full=svd_full, positions=positions, digit_ans_pos=digit_ans_pos, digit_naive_ans_pos=digit_naive_ans_pos)\n",
    "\n",
    "plot_attention_patterns(n=n, model=model, positions=positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36969b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This outputs:\n",
    "\n",
    "    Accuracy per position\n",
    "\"\"\"\n",
    "\n",
    "a = torch.tensor([])\n",
    "\n",
    "toLoad = directory + mdd\n",
    "\n",
    "### Ablation specification\n",
    "\n",
    "## Attention \n",
    "\n",
    "M_apply = False\n",
    "n_ab_head = []\n",
    "# ab_headrow = [[[7, 8, 9], [7, 8, 9]], [[0, 1, 2, 3, 4, 5, 6], [0, 1, 2, 3, 4, 5, 6]], [[0, 1, 2, 3, 4, 5, 6], [0, 1, 2, 3, 4, 5, 6]]]\n",
    "# ab_headrow = [[[7, 8, 9], [7, 8, 9]], [[],[]]]\n",
    "ab_headrow = []\n",
    "ab_head = torch.eye(2,2)\n",
    "\n",
    "ablation_attention = [M_apply, n_ab_head, ab_head, ab_headrow]\n",
    "\n",
    "## FFN\n",
    "\n",
    "List_Neurons = []\n",
    "\n",
    "ablation_ffn = List_Neurons\n",
    "\n",
    "## Decoder \n",
    "\n",
    "n_ab_ffn = []\n",
    "n_ab_att = []\n",
    "\n",
    "ablation_decoder = [n_ab_att, n_ab_ffn]\n",
    "\n",
    "ablations = [ablation_attention, ablation_ffn, ablation_decoder]\n",
    "\n",
    "model = make_model(vocab, N = n, d_model = 128, d_ff = d_ff, h = 2, dropout=0.1, ablation_data=ablations)\n",
    "model.load_state_dict(torch.load(toLoad, map_location='cpu')[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "ix = torch.randint(len(data_f), size = (20000,))\n",
    "inputs = data_f[ix]\n",
    "targets = target_f[ix]\n",
    "\n",
    "seq_len = inputs.shape[-1]\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "out = model(inputs, mask)\n",
    "\n",
    "a = torch.cat((a, (sum((torch.argmax(out[i, -4:-1, :].detach().to('cpu'), -1) == targets[i, -4:-1]).float() for i in range(len(inputs))) / len(inputs)).unsqueeze(0)), 0)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c2c6cc",
   "metadata": {},
   "source": [
    "# Table 4 (left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This outputs:\n",
    "\n",
    "    Accuracy, non-corrected and corrected after ablating chosen part of network zero-ablated. \n",
    "    We correct answers manually to see whether the original ones where off by one either \n",
    "    by forgetting a carried one or adding one where it shouldnt have, this is the list vcorr. \n",
    "    This means: For non-carry over sums we subtract 1 from each position and when a carried one is needed we add it.  \n",
    "\n",
    "    ### By default we have ablated the final MLP! ###\n",
    "\"\"\"\n",
    "\n",
    "scores_z = torch.tensor([])\n",
    "\n",
    "for k in range(5):\n",
    "\n",
    "    ix = torch.randint(len(data_f), size = (20000,))\n",
    "    data_ff = data_f[ix]\n",
    "    target_ff = target_f[ix]\n",
    "\n",
    "    pos_nc = np.argwhere(sum((data_ff[:, j] + data_ff[:, j+4] >= 10) for j in range(3)) < 1)\n",
    "    pos_c1 = np.argwhere((data_ff[:, 1] + data_ff[:, 1+4] >= 10) & (sum((data_ff[:, j] + data_ff[:, j+4] >= 10).float() for j in np.delete(np.arange(3), 1)) < 1))\n",
    "    pos_c2 = np.argwhere((data_ff[:, 2] + data_ff[:, 2+4] >= 10) & (data_ff[:, 1] + data_ff[:, 5] < 9))\n",
    "    pos_2c = np.argwhere(sum((data_ff[:, j] + data_ff[:, j+4] >= 10) for j in range(3)) == 2)\n",
    "    pos_2cp = np.argwhere((data_ff[:, 1] + data_ff[:, 5] == 9) & (data_ff[:, 2] + data_ff[:, 6] >= 10))\n",
    "\n",
    "    tasks_src = [data_ff[pos_nc[0]], data_ff[pos_c1[0]], data_ff[pos_c2[0]], data_ff[pos_2c[0]], data_ff[pos_2cp[0]]]\n",
    "    tasks_tgt = [target_ff[pos_nc[0]], target_ff[pos_c1[0]], target_ff[pos_c2[0]], target_ff[pos_2c[0]], target_ff[pos_2cp[0]]]\n",
    "\n",
    "    vcorr = [torch.tensor([1, 1, 1]), torch.tensor([-1, 1, 1]), torch.tensor([1, -1, 1]), torch.tensor([-1, -1, 1]), torch.tensor([-1, -1, 1])]\n",
    "\n",
    "    inputs = tasks_src[k]\n",
    "    targets = tasks_tgt[k]\n",
    "    \n",
    "    scores = torch.tensor([])\n",
    "\n",
    "    for l in range(1):\n",
    "\n",
    "        ### Ablation specification\n",
    "\n",
    "        ## Attention \n",
    "\n",
    "        M_apply = False\n",
    "        n_ab_head = []\n",
    "        # ab_headrow = [[[7, 8, 9], [7, 8, 9]], [[0, 1, 2, 3, 4, 5, 6], [0, 1, 2, 3, 4, 5, 6]], [[0, 1, 2, 3, 4, 5, 6], [0, 1, 2, 3, 4, 5, 6]]]\n",
    "        # ab_headrow = [[[7, 8, 9], [7, 8, 9]], [[],[]]]\n",
    "        ab_headrow = []\n",
    "        ab_head = torch.eye(2,2)\n",
    "\n",
    "        ablation_attention = [M_apply, n_ab_head, ab_head, ab_headrow]\n",
    "\n",
    "        ## FFN\n",
    "\n",
    "        List_Neurons = []\n",
    "\n",
    "        ablation_ffn = List_Neurons\n",
    "\n",
    "        ## Decoder \n",
    "\n",
    "        n_ab_ffn = [1] ### Final MLP is ablated\n",
    "        n_ab_att = []\n",
    "\n",
    "        ablation_decoder = [n_ab_att, n_ab_ffn]\n",
    "\n",
    "        ablations = [ablation_attention, ablation_ffn, ablation_decoder]\n",
    "\n",
    "        model = make_model(vocab, N = n, d_model = 128, d_ff = d_ff, h = 2, dropout=0.1, ablation_data=ablations)\n",
    "        model.load_state_dict(torch.load(toLoad, map_location='cpu')[\"model\"])\n",
    "        model.eval()\n",
    "\n",
    "        seq_len = inputs.shape[-1]\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "        out = model(inputs, mask)\n",
    "\n",
    "        pre = torch.argmax(out[:, -4:-1, :].detach().to('cpu'), -1)\n",
    "\n",
    "        bp = sum((pre[i] == targets[i, -4:-1]).float() for i in range(len(inputs))) / len(inputs)\n",
    "\n",
    "        bcorr = sum(((pre[i] - vcorr[k]) % 10 == targets[i, -4:-1]).float() for i in range(len(inputs))) / len(inputs)\n",
    "\n",
    "        scores = torch.cat((scores, torch.cat((bp.unsqueeze(0), bcorr.unsqueeze(0)), 0)), 0)\n",
    "        \n",
    "    scores_z = torch.cat((scores_z, scores.unsqueeze(0)), 0)  \n",
    "\n",
    "scores_z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9949d13",
   "metadata": {},
   "source": [
    "# Table 4 (right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f2644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This outputs:\n",
    "    \n",
    "    Dissecting the MLP and computing the accuracy after ablating a specific set of Neurons (List_Neurons)\n",
    "    We go through the data twice first to find the relevant Neurons and then to rerun the model with those \n",
    "    neurons zero-ablated. \n",
    "\"\"\"\n",
    "\n",
    "List_Neurons = []\n",
    "\n",
    "for _ in range(2):\n",
    "    ### Ablation specification\n",
    "\n",
    "    ## Attention \n",
    "\n",
    "    M_apply = False\n",
    "    n_ab_head = []\n",
    "    # ab_headrow = [[[7, 8, 9], [7, 8, 9]], [[0, 1, 2, 3, 4, 5, 6], [0, 1, 2, 3, 4, 5, 6]], [[0, 1, 2, 3, 4, 5, 6], [0, 1, 2, 3, 4, 5, 6]]]\n",
    "    # ab_headrow = [[[7, 8, 9], [7, 8, 9]], [[],[]]]\n",
    "    ab_headrow = []\n",
    "    ab_head = torch.eye(2,2)\n",
    "\n",
    "    ablation_attention = [M_apply, n_ab_head, ab_head, ab_headrow]\n",
    "\n",
    "    ## FFN\n",
    "\n",
    "    ablation_ffn = List_Neurons\n",
    "\n",
    "    ## Decoder \n",
    "\n",
    "    n_ab_ffn = []\n",
    "    n_ab_att = []\n",
    "\n",
    "    ablation_decoder = [n_ab_att, n_ab_ffn]\n",
    "\n",
    "    ablations = [ablation_attention, ablation_ffn, ablation_decoder]\n",
    "\n",
    "    model = make_model(vocab, N = n, d_model = 128, d_ff = d_ff, h = 2, dropout=0.1, ablation_data=ablations)\n",
    "    model.load_state_dict(torch.load(toLoad, map_location='cpu')[\"model\"])\n",
    "    model.eval()\n",
    "\n",
    "    ix = torch.randint(len(data_f), size = (100000,))\n",
    "    data_ff = data_f[ix]\n",
    "    target_ff = target_f[ix]\n",
    "\n",
    "    pos_nc = np.argwhere(sum((data_ff[:, j] + data_ff[:, j+4] >= 10) for j in range(3)) < 1)\n",
    "    pos_c1 = np.argwhere((data_ff[:, 1] + data_ff[:, 1+4] >= 10) & (sum((data_ff[:, j] + data_ff[:, j+4] >= 10).float() for j in np.delete(np.arange(3), 1)) < 1))\n",
    "    pos_c2 = np.argwhere((data_ff[:, 2] + data_ff[:, 2+4] >= 10) & (data_ff[:, 1] + data_ff[:, 5] < 9))\n",
    "    pos_2c = np.argwhere(sum((data_ff[:, j] + data_ff[:, j+4] >= 10) for j in range(3)) == 2)\n",
    "    pos_2cp = np.argwhere((data_ff[:, 1] + data_ff[:, 5] == 9) & (data_ff[:, 2] + data_ff[:, 6] >= 10))\n",
    "\n",
    "    tasks_src = [data_ff[pos_nc[0]], data_ff[pos_c1[0]], data_ff[pos_c2[0]], data_ff[pos_2c[0]], data_ff[pos_2cp[0]]]\n",
    "    tasks_tgt = [target_ff[pos_nc[0]], target_ff[pos_c1[0]], target_ff[pos_c2[0]], target_ff[pos_2c[0]], target_ff[pos_2cp[0]]]\n",
    "\n",
    "    vcorr = [torch.tensor([1, 1, 1]), torch.tensor([-1, 1, 1]), torch.tensor([1, -1, 1]), torch.tensor([-1, -1, 1]), torch.tensor([-1, -1, 1])]\n",
    "\n",
    "    a = torch.tensor([])\n",
    "    z_out = []\n",
    "    for k in range(len(tasks_src)):\n",
    "\n",
    "        inputs = tasks_src[k]\n",
    "        targets = tasks_tgt[k]\n",
    "\n",
    "        seq_len = inputs.shape[-1]\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "        out = model(inputs, mask)\n",
    "\n",
    "        ap = sum((torch.argmax(out[i, -4:-1, :].detach().to('cpu'), -1) == targets[i, -4:-1]).float() for i in range(len(inputs))) / len(inputs)\n",
    "\n",
    "        acorr = sum(((torch.argmax(\n",
    "                        out[i, -4:-1, :].detach().to('cpu'), -1) - vcorr[k]) % 10 == targets[i, -4:-1]).float() for i in range(len(inputs))) / len(inputs)\n",
    "\n",
    "        a = torch.cat((a, torch.cat((ap.unsqueeze(0), acorr.unsqueeze(0)), 0)), 0)\n",
    "\n",
    "        z = model.decoder.layers[1].ffn.out[:, -4:-1, :].mean(0).clone().detach()[0]\n",
    "        z_out.append(z)\n",
    "    Neur = []\n",
    "    for i in range(1, 5):  \n",
    "        zg = torch.argwhere(z_out[i] > z_out[0]).squeeze(-1)\n",
    "        for j in range(len(zg)):\n",
    "            if zg[j] not in Neur:\n",
    "                Neur.append(zg[j].item())\n",
    "    List_Neurons = Neur\n",
    "    print(len(List_Neurons), torch.tensor(List_Neurons))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d09901f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4188f393c34fa2b8c99d7feb044de464e631de5d42645b8d328d986184b8407"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
