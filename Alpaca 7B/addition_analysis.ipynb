{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from transformers import LlamaTokenizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "        \"pgf.texsystem\": \"pdflatex\",\n",
    "        \"text.usetex\": True,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": \"Helvetica\",\n",
    "        'pgf.rcfonts': False,\n",
    "    })\n",
    "\n",
    "plt.style.use('seaborn-v0_8-ticks')\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "\n",
    "plt.locator_params(nbins=4)\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading data. Can use the unablated or ablated data. In the latter case we zero ablated heads 6:23 and 13:15. \n",
    "\n",
    "input = torch.load('addition/test/inputs.pt', map_location='cpu').int()\n",
    "gen = torch.load('addition/test/generations.pt', map_location='cpu')\n",
    "data = torch.load('addition/test/data.pt', map_location='cpu') ## input_text, correctness, acc, counter, av_att, std_att\n",
    "\n",
    "input_text, correctness, acc, counter, av_att, std_att = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting the count of heads with (part of) a staircase type attention pattern.\n",
    "\n",
    "plt.imshow(counter)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Relevancy of heads and percentage the first leading heads take up and how many of 1024 get turned on during inference.\n",
    "\n",
    "sorted_ = torch.sort(counter.view(-1), descending=True)\n",
    "\n",
    "sum(sorted_[0][:2]) / sum(sorted_[0]), sum(sorted_[0] != 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy of the model on the addition task. \n",
    "\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating ticks for plotting\n",
    "\n",
    "example = 0\n",
    "seq_len = input[example].shape[0]\n",
    "\n",
    "sorted_ = torch.sort(counter.view(-1), descending=True)\n",
    "\n",
    "l0 = [min(torch.argwhere(input[i, :] == 29901)) for i in range(input.shape[0])] # To get tokens of first integer. 29901 decodes to \":\".\n",
    "l2 = [torch.argwhere(input[i, ] == 353) for i in range(input.shape[0])] # Final token before addition sum starts. 353 decodes to \"=\".\n",
    "\n",
    "ll0 = l0[example].item() + 2\n",
    "ll2 = l2[example].item() \n",
    "\n",
    "ticks = [('$*$' if (tokenizer.decode(input[example][i]).isdigit()) else tokenizer.decode(input[example][i])) for i in range(ll0, ll2+1)] + ['$\\cdot$'] * 7 + ['$*$'] * 5\n",
    "\n",
    "## Plotting the attention patters, mean and variance.\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
    "fig.tight_layout()\n",
    "\n",
    "for i in range(2):\n",
    "    coord = torch.argwhere(counter == sorted_[0][i])[0]\n",
    "    ax[i, 0].imshow(av_att[i], cmap='Blues')\n",
    "\n",
    "    ax[i, 0].set_xticks(range(len(ticks)), ticks, rotation='vertical')\n",
    "    ax[i, 0].set_yticks(range(len(ticks)), ticks)\n",
    "    ax[i, 0].set_title('\\\\rm Mean attention')\n",
    "\n",
    "\n",
    "    ax[i, 1].imshow(std_att[i]**2, cmap='Blues')\n",
    "\n",
    "    ax[i, 1].set_xticks(range(len(ticks)), ticks, rotation='vertical')\n",
    "    ax[i, 1].set_yticks(range(len(ticks)), ticks)\n",
    "    ax[i, 1].set_title('\\\\rm Variance attention')\n",
    "\n",
    "\n",
    "    row = f'$\\\\rm Head\\;{{a}}:{{b}}$'.format(a = coord[0], b = coord[1])\n",
    "\n",
    "    ax[i, 0].annotate(row, xy=(0, 0), xytext=(-ax[i, 0].yaxis.labelpad - 5, 0),\n",
    "                xycoords=ax[i, 0].yaxis.label, textcoords='offset points',\n",
    "                size='large', ha='right', va='center')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
